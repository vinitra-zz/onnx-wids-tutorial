{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch Bert Model for High Performance in ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll learn how to load a BERT model from PyTorch, convert it to ONNX, and inference it for high performance using ONNX Runtime with transformer optimization. In the following sections, we are going to use the BERT model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. BERT SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "### Tutorial Roadmap\n",
    "\n",
    "During this tutorial, we will:\n",
    "0. Install a few prerequisite packages (PyTorch, Transformers, TorchVision, wget).\n",
    "1. Load a pretrained BERT SQuAD model from a source PyTorch implementation.\n",
    "2. Export our PyTorch BERT model to ONNX.\n",
    "3. Compare inference on our models for PyTorch and ONNX Runtime.\n",
    "4. Optimize our inference with execution providers using the ONNX Go Live (OLive) tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "First you need to check if the following packages exist and install them if needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\anaconda3\\lib\\site-packages (3.2)\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.3.1 in c:\\users\\viswamy\\appdata\\roaming\\python\\python36\\site-packages (1.3.1+cu92)\n",
      "Requirement already satisfied: torchvision==0.4.2+cpu in c:\\users\\viswamy\\appdata\\roaming\\python\\python36\\site-packages (0.4.2+cpu)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from torch==1.3.1) (1.18.1)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from torchvision==0.4.2+cpu) (1.13.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\anaconda3\\lib\\site-packages (from torchvision==0.4.2+cpu) (5.2.0)\n",
      "Requirement already satisfied: transformers==2.5.1 in c:\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in c:\\anaconda3\\lib\\site-packages (from transformers==2.5.1) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda3\\lib\\site-packages (from transformers==2.5.1) (4.42.1)\n",
      "Requirement already satisfied: boto3 in c:\\anaconda3\\lib\\site-packages (from transformers==2.5.1) (1.9.16)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda3\\lib\\site-packages (from transformers==2.5.1) (2020.2.20)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from transformers==2.5.1) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\anaconda3\\lib\\site-packages (from transformers==2.5.1) (0.0.38)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from transformers==2.5.1) (2.22.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\anaconda3\\lib\\site-packages (from transformers==2.5.1) (0.1.85)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from transformers==2.5.1) (1.18.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\anaconda3\\lib\\site-packages (from boto3->transformers==2.5.1) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\anaconda3\\lib\\site-packages (from boto3->transformers==2.5.1) (0.1.13)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.16 in c:\\anaconda3\\lib\\site-packages (from boto3->transformers==2.5.1) (1.12.16)\n",
      "Requirement already satisfied: click in c:\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.5.1) (7.0)\n",
      "Requirement already satisfied: joblib in c:\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.5.1) (0.14.1)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.5.1) (1.13.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->transformers==2.5.1) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->transformers==2.5.1) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->transformers==2.5.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->transformers==2.5.1) (2.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.16->boto3->transformers==2.5.1) (2.8.1)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.16->boto3->transformers==2.5.1) (0.14)\n",
      "Requirement already satisfied: psutil in c:\\anaconda3\\lib\\site-packages (5.6.7)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install wget\n",
    "!{sys.executable} -m pip install --user torch==1.3.1 torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!{sys.executable} -m pip install transformers==2.5.1\n",
    "!{sys.executable} -m pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained BERT model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by downloading the data files and store them in the specified ``pytorch_output`` and ``pytorch_squad`` directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to store predict file\n",
    "output_dir = \"./pytorch_output\"\n",
    "cache_dir = \"./pytorch_squad\"\n",
    "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
    "# create cache dir\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "    \n",
    "# Download the file\n",
    "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "if not os.path.exists(predict_file):\n",
    "    import wget\n",
    "    print(\"Start downloading predict file.\")\n",
    "    wget.download(predict_file_url, predict_file)\n",
    "    print(\"Predict file downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify some relevant model config / hyperparameter variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables. As an example, we used batch size 1 and max sequence length 128. \n",
    "model_type = \"bert\"\n",
    "model_name_or_path = \"bert-base-cased\"\n",
    "max_seq_length = 128\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "eval_batch_size = 1\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # The hardware you'd like to use to run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start to load our BERT model into PyTorch from our pretrained files. This step could take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "100%|██████████| 48/48 [00:03<00:00, 12.40it/s]\n",
      "convert squad examples to features: 100%|██████████| 3/3 [00:00<00:00, 124.98it/s]\n",
      "add example index and unique id: 100%|██████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# The following code is adapted from HuggingFace transformers\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py#L290\n",
    "\n",
    "from transformers import (WEIGHTS_NAME, BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "from torch.utils.data import (DataLoader, SequentialSampler)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path,\n",
    "                                    from_tf=False,\n",
    "                                    config=config,\n",
    "                                    cache_dir=cache_dir)\n",
    "\n",
    "# Load and Convert the examples from the downloaded predict file into a list of features \n",
    "# that can be directly given as input to a model.\n",
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "\n",
    "processor = SquadV2Processor()\n",
    "examples = processor.get_dev_examples(None, filename=predict_file)\n",
    "\n",
    "from transformers import squad_convert_examples_to_features\n",
    "features, dataset = squad_convert_examples_to_features( \n",
    "            examples=examples[:3], # convert only 3 examples for demo\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Problem Formulation\n",
    "\n",
    "**Input**: Context paragraph and questions\n",
    "\n",
    "**Task**: For each question about the context paragraph, the model predicts a start and an end token from the paragraph that most likely answers the questions.\n",
    "\n",
    "**Output**: The best answer for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qas_id:  56beb3083aeaaa14008c923e \n",
      "\n",
      "context_text:  Despite waiving longtime running back DeAngelo Williams and losing top wide receiver Kelvin Benjamin to a torn ACL in the preseason, the Carolina Panthers had their best regular season in franchise history, becoming the seventh team to win at least 15 regular season games since the league expanded to a 16-game schedule in 1978. Carolina started the season 14–0, not only setting franchise records for the best start and the longest single-season winning streak, but also posting the best start to a season by an NFC team in NFL history, breaking the 13–0 record previously shared with the 2009 New Orleans Saints and the 2011 Green Bay Packers. With their NFC-best 15–1 regular season record, the Panthers clinched home-field advantage throughout the NFC playoffs for the first time in franchise history. Ten players were selected to the Pro Bowl (the most in franchise history) along with eight All-Pro selections. \n",
      "\n",
      "question_text:  Who had the best record in the NFC? \n",
      "\n",
      "answer_text:  None \n",
      "\n",
      "title:  Super_Bowl_50 \n",
      "\n",
      "is_impossible:  False \n",
      "\n",
      "answers:  [{'answer_start': 137, 'text': 'Carolina Panthers'}, {'answer_start': 695, 'text': 'the Panthers'}, {'answer_start': 330, 'text': 'Carolina'}]\n"
     ]
    }
   ],
   "source": [
    "example_index = 200\n",
    "example = examples[example_index]\n",
    "print('qas_id: ', example.qas_id, '\\n')\n",
    "print('context_text: ', example.context_text, '\\n')\n",
    "print('question_text: ', example.question_text, '\\n')\n",
    "print('answer_text: ', example.answer_text, '\\n')\n",
    "print('title: ', example.title, '\\n')\n",
    "print('is_impossible: ', example.is_impossible, '\\n')\n",
    "print('answers: ', example.answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export the loaded model ##\n",
    "Once the model is loaded, we can export the loaded PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation {} *****\n",
      "  Num examples =  6\n",
      "  Batch size =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viswamy\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\onnx\\symbolic_opset9.py:427: UserWarning: ONNX export squeeze with negative axis -1 might cause the onnx model to be incorrect. Negative axis is not supported in ONNX. Axis is converted to 2 based on input shape at export time. Passing an tensor of different rank in execution will be incorrect.\n",
      "  \"Passing an tensor of different rank in execution will be incorrect.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported at  ./pytorch_squad/bert-base-cased-squad.onnx\n"
     ]
    }
   ],
   "source": [
    "# Eval!\n",
    "print(\"***** Running evaluation {} *****\")\n",
    "print(\"  Num examples = \", len(dataset))\n",
    "print(\"  Batch size = \", eval_batch_size)\n",
    "\n",
    "# create output dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "output_model_path = './pytorch_squad/bert-base-cased-squad.onnx'    \n",
    "inputs = {}\n",
    "outputs= {}\n",
    "# Get the first batch of data to run the model and export it to ONNX\n",
    "batch = dataset[0]\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "batch = tuple(t.to(device) for t in batch)\n",
    "inputs = {\n",
    "    'input_ids':      batch[0].reshape(1, 128),                         # using batch size = 1 here. Adjust as needed.\n",
    "    'attention_mask': batch[1].reshape(1, 128),\n",
    "    'token_type_ids': batch[2].reshape(1, 128)\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "    torch.onnx.export(model,                                            # model being run\n",
    "                      (inputs['input_ids'],                             # model input (or a tuple for multiple inputs)\n",
    "                       inputs['attention_mask'], \n",
    "                       inputs['token_type_ids']), \n",
    "                      output_model_path,                                # where to save the model (can be a file or file-like object)\n",
    "                      opset_version=11,                                 # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                      input_names=['input_ids',                         # the model's input names\n",
    "                                   'input_mask', \n",
    "                                   'segment_ids'],\n",
    "                      output_names=['start', 'end'],                    # the model's output names\n",
    "                      dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                    'input_mask' : symbolic_names,\n",
    "                                    'segment_ids' : symbolic_names,\n",
    "                                    'start' : symbolic_names,\n",
    "                                    'end' : symbolic_names})\n",
    "    print(\"Model exported at \", output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference the Exported Model with ONNX Runtime ##\n",
    "\n",
    "#### Install ONNX Runtime\n",
    "Install ONNX Runtime if you haven't done so already. Make sure to install the correct package from PyPi -- `onnxruntime` to use CPU features, or `onnxruntime-gpu` to use GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: onnxruntime in c:\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.18.0 in c:\\anaconda3\\lib\\site-packages (from onnxruntime) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: onnx>=1.2.3 in c:\\anaconda3\\lib\\site-packages (from onnxruntime) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\anaconda3\\lib\\site-packages (from onnx>=1.2.3->onnxruntime) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf in c:\\anaconda3\\lib\\site-packages (from onnx>=1.2.3->onnxruntime) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: typing>=3.6.4 in c:\\anaconda3\\lib\\site-packages (from onnx>=1.2.3->onnxruntime) (3.6.6)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.2.1 in c:\\anaconda3\\lib\\site-packages (from onnx>=1.2.3->onnxruntime) (3.6.5)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\viswamy\\appdata\\roaming\\python\\python36\\site-packages (from protobuf->onnx>=1.2.3->onnxruntime) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "ONNXRUNTIME = 'onnxruntime'\n",
    "# Install ONNX Runtime\n",
    "if torch.cuda.is_available():\n",
    "    ## Install onnxruntime-gpu if cuda is available\n",
    "    ONNXRUNTIME = 'onnxruntime-gpu'\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install -U $ONNXRUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inference our model with ONNX Runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime inference time:  0.13757848739624023\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as rt  \n",
    "import time\n",
    "import psutil\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "\n",
    "# Set graph optimization level to ORT_ENABLE_EXTENDED to enable bert optimization. This is enabled on default.\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "# The following settings enables OpenMP, which is required to get best performance for CPU inference of Bert models.\n",
    "sess_options.intra_op_num_threads=1\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(psutil.cpu_count(logical=True))\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "session = rt.InferenceSession(output_model_path, sess_options)\n",
    "\n",
    "# evaluate the model\n",
    "start = time.time()\n",
    "res = session.run(None, {\n",
    "          'input_ids': inputs['input_ids'].cpu().numpy(),\n",
    "          'input_mask': inputs['attention_mask'].cpu().numpy(),\n",
    "          'segment_ids': inputs['token_type_ids'].cpu().numpy()\n",
    "        })\n",
    "end = time.time()\n",
    "print(\"ONNX Runtime inference time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get comparative performance numbers from the original PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Inference time =  0.28899693489074707\n",
      "***** Verifying correctness *****\n",
      "PyTorch and ORT matching numbers: True\n",
      "PyTorch and ORT matching numbers: True\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "outputs = model(**inputs)\n",
    "end = time.time()\n",
    "print(\"PyTorch Inference time = \", end - start)\n",
    "\n",
    "print(\"***** Verifying correctness *****\")\n",
    "import numpy as np\n",
    "for i in range(2):\n",
    "    print('PyTorch and ORT matching numbers:', np.allclose(res[i], outputs[i].cpu().detach().numpy(), rtol=1e-04, atol=1e-05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tune Best Performance with OLive tool\n",
    "\n",
    "To get a better performance, we can tune parameters and try different execution providers in ONNX Runtime. The [OLive(ONNX Go Live) tool](https://github.com/microsoft/OLive) provides docker images as well as a web app to help finding the best parameters and execution provider for a specific model. \n",
    "\n",
    "![OLive-Demo](img/OLive_demo_pic.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
