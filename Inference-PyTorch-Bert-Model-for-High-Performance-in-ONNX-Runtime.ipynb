{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch Bert Model for High Performance in ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll be introduced to how to load a Bert model from PyTorch, convert it to ONNX, and inference it for high performance using ONNX Runtime with transformer optimization. In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "First you need to check if the following packages exist and install them if needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.2)\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.3.1 in c:\\users\\ziyl\\appdata\\roaming\\python\\python37\\site-packages (1.3.1+cpu)\n",
      "Requirement already satisfied: torchvision==0.4.2+cpu in c:\\users\\ziyl\\appdata\\roaming\\python\\python37\\site-packages (0.4.2+cpu)\n",
      "Requirement already satisfied: numpy in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from torch==1.3.1) (1.18.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from torchvision==0.4.2+cpu) (6.2.1)\n",
      "Requirement already satisfied: six in c:\\users\\ziyl\\appdata\\roaming\\python\\python37\\site-packages (from torchvision==0.4.2+cpu) (1.12.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (2019.12.19)\n",
      "Requirement already satisfied: boto3 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (1.10.41)\n",
      "Requirement already satisfied: requests in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (4.40.2)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: numpy in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.41 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.13.41)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: six in c:\\users\\ziyl\\appdata\\roaming\\python\\python37\\site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.14.0,>=1.13.41->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in c:\\users\\ziyl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.14.0,>=1.13.41->boto3->transformers) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install wget\n",
    "!{sys.executable} -m pip install --user torch==1.3.1 torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained Bert model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by downloading the data files and store them in the specified location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to store predict file\n",
    "output_dir = \"./pytorch_output\"\n",
    "cache_dir = \"./pytorch_squad\"\n",
    "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
    "# create cache dir\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "    \n",
    "# Download the file\n",
    "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "if not os.path.exists(predict_file):\n",
    "    import wget\n",
    "    print(\"Start downloading predict file.\")\n",
    "    wget.download(predict_file_url, predict_file)\n",
    "    print(\"Predict file downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify some model config variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables. As an example, we used batch size 1 and max sequence length 128. \n",
    "model_type = \"bert\"\n",
    "model_name_or_path = \"bert-base-cased\"\n",
    "max_seq_length = 128\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "eval_batch_size = 1\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # The hardware you'd like to use to run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to load model from pretrained. This step could take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0302 18:07:44.319301 1951928 file_utils.py:32] PyTorch version 1.3.1+cpu available.\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ziyl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I0302 18:07:49.951069 1951928 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at ./pytorch_squad\\b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e\n",
      "I0302 18:07:49.951069 1951928 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0302 18:07:50.452054 1951928 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at ./pytorch_squad\\5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0302 18:07:50.811056 1951928 modeling_utils.py:403] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at ./pytorch_squad\\35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I0302 18:07:55.374041 1951928 modeling_utils.py:475] Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "I0302 18:07:55.374041 1951928 modeling_utils.py:478] Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:04<00:00, 11.67it/s]\n",
      "Converting examples to features:  91%|███████████████████████████████████████▎   | 9664/10570 [01:10<00:06, 142.11it/s]"
     ]
    }
   ],
   "source": [
    "# The following code is adapted from HuggingFace transformers\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py#L290\n",
    "\n",
    "from transformers import (WEIGHTS_NAME, BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "from torch.utils.data import (DataLoader, SequentialSampler)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path,\n",
    "                                    from_tf=False,\n",
    "                                    config=config,\n",
    "                                    cache_dir=cache_dir)\n",
    "\n",
    "# Load and Convert the examples from the downloaded predict file into a list of features \n",
    "# that can be directly given as input to a model.\n",
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "\n",
    "processor = SquadV2Processor()\n",
    "examples = processor.get_dev_examples(None, filename=predict_file)\n",
    "\n",
    "from transformers import squad_convert_examples_to_features\n",
    "features, dataset = squad_convert_examples_to_features( \n",
    "            examples=examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt'\n",
    "        )\n",
    "\n",
    "# create output dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# multi-gpu evaluate\n",
    "if n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "    model = torch.nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export the loaded model ##\n",
    "Once the model is loaded, we can export the loaded PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval!\n",
    "print(\"***** Running evaluation {} *****\")\n",
    "print(\"  Num examples = \", len(dataset))\n",
    "print(\"  Batch size = \", eval_batch_size)\n",
    "\n",
    "output_model_path = './pytorch_squad/bert-base-cased-squad.onnx'    \n",
    "inputs = {}\n",
    "outputs= {}\n",
    "# Get the first batch of data to run the model and export it to ONNX\n",
    "batch = dataset[0]\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "batch = tuple(t.to(device) for t in batch)\n",
    "inputs = {\n",
    "    'input_ids':      batch[0].reshape(1, 128),                         # using batch size = 1 here. Adjust as needed.\n",
    "    'attention_mask': batch[1].reshape(1, 128),\n",
    "    'token_type_ids': batch[2].reshape(1, 128)\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "    torch.onnx.export(model,                                            # model being run\n",
    "                      (inputs['input_ids'],                             # model input (or a tuple for multiple inputs)\n",
    "                       inputs['attention_mask'], \n",
    "                       inputs['token_type_ids']), \n",
    "                      output_model_path,                                # where to save the model (can be a file or file-like object)\n",
    "                      opset_version=11,                                 # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                      input_names=['input_ids',                         # the model's input names\n",
    "                                   'input_mask', \n",
    "                                   'segment_ids'],\n",
    "                      output_names=['start', 'end'],                    # the model's output names\n",
    "                      dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                    'input_mask' : symbolic_names,\n",
    "                                    'segment_ids' : symbolic_names,\n",
    "                                    'start' : symbolic_names,\n",
    "                                    'end' : symbolic_names})\n",
    "    print(\"Model exported at \", output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference the Exported Model with ONNX Runtime ##\n",
    "\n",
    "#### Install ONNX Runtime\n",
    "Install ONNX Runtime if you haven't done so already. \n",
    "\n",
    "Install `onnxruntime` to use CPU features, or `onnxruntime-gpu` to use GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNXRUNTIME = 'onnxruntime'\n",
    "# Install ONNX Runtime\n",
    "if torch.cuda.is_available():\n",
    "    ## Install onnxruntime-gpu if cuda is available\n",
    "    ONNXRUNTIME = 'onnxruntime-gpu'\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install -U $ONNXRUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inference the model with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt  \n",
    "import time\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "\n",
    "# Set graph optimization level to ORT_ENABLE_EXTENDED to enable bert optimization.\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "# Uncomment the following line to store the optimized graph to desired location.\n",
    "# sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model.onnx\")\n",
    "session = rt.InferenceSession(output_model_path, sess_options)\n",
    "\n",
    "# evaluate the model\n",
    "start = time.time()\n",
    "res = session.run(None, {\n",
    "          'input_ids': inputs['input_ids'].cpu().numpy(),\n",
    "          'input_mask': inputs['attention_mask'].cpu().numpy(),\n",
    "          'segment_ids': inputs['token_type_ids'].cpu().numpy()\n",
    "        })\n",
    "end = time.time()\n",
    "print(\"ONNX Runtime inference time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get perf numbers from the original PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "outputs = model(**inputs)\n",
    "end = time.time()\n",
    "print(\"PyTorch Inference time = \", end - start)\n",
    "\n",
    "print(\"***** Verifying correctness *****\")\n",
    "import numpy as np\n",
    "for i in range(2):\n",
    "    print('PyTorch and ORT matching numbers:', np.allclose(res[i], outputs[i].cpu().detach().numpy(), rtol=1e-04, atol=1e-05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tune Best Performance with OLive tool\n",
    "\n",
    "To get a better performance, we can tune parameters and try different execution providers in ONNX Runtime. The [OLive(ONNX Go Live) tool](https://github.com/microsoft/OLive) provides docker images as well as a web app to help finding the best parameters and execution provider for a specific model. \n",
    "\n",
    "![OLive-Demo](img/OLive_demo_pic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
